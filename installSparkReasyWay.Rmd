---
title: "Install and Run SparkR - easy way"
output: html_document
---

## Requirement

First, you must have R and java installed. This is a bit 
out the scope of this note, but Let me cover few thinigs.

### R



Next, you have to see if you can install R interface to java. this can
be done by calling

```
install.packages("rJava")
```

Depending or your Operating System this can produce an error, and some
extra action would be required. Please google something like `rJava
install error ubuntu` etc. 

### R and rJava on Mac

https://github.com/snowflakedb/dplyr-snowflakedb/wiki/Configuring-R-rJava-RJDBC-on-Mac-OS-X

You can also install few packages we may use in presentation. Simply call:

```
install.packages(c("rmarkdown", "ggplot2", "magrittr", "whisker", "data.table", "reshape2"))
```

* `rmarkdown`: package that makes possible to create this document
* `ggplot2`: popular graphing package,
* `magrittr`: nicer functions chaining
* `whisker`: it's Movember
* `data.table`, `reshape2`: an alternative for "data.frame"

## Downloading Spark

The easiest way to downloading spark is getting pre-bulid version from
[http://spark.apache.org/downloads.html]. 

<center>

<img src="pic3.png" height="200px"/> 

</center>

### Mac
https://www.r-bloggers.com/six-lines-to-install-and-start-sparkr-on-mac-os-x-yosemite/

## SparkR test drive

The way we use SparkR here is far from being en example of _best
practice_. Not only because it does not run on more than one computer,
but also because we isolate the SparkR package from other packages by
hardcoding library path. This should help you set up you Spark(R) fast
for test drive.

Lets say you have downloaded and uncompress it to the folder

`/Users/bartek/programs/spark-2.3.0-bin-hadoop2.7`

So every time you see this path please change it to the one you have
(windows users have to probably change also slashes `/` to backslashes `\` and add something like `/C/`)


Then try to run the following code:
```{r}
#spark_path <- '~/programs/spark-2.3.0-bin-hadoop2.7'
library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib")))

spark_path <- strsplit(system("brew info apache-spark",intern=T)[4],' ')[[1]][1] 
if (nchar(Sys.getenv("SPARK_HOME")) < 1) {
  Sys.setenv(SPARK_HOME = spark_path)
}
library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"), "libexec", "R", "lib")))
sparkR.session(master = "local[*]", sparkConfig = list(spark.driver.memory = "2g"))
```

```{r}
#mac:
spark_path <- strsplit(system("brew info apache-spark",intern=T)[4],' ')[[1]][1] 
.libPaths(c(file.path(spark_path,"libexec", "R", "lib"), .libPaths())) # Navigate to SparkR folder
library(SparkR)
sparkR.session(master = "local[*]", sparkConfig = list(spark.driver.memory = "2g"))
```

```{r}
sc <- sparkR.init()
sqlContext <- sparkRSQL.init(sc)
```

```{r}
library(SparkR, lib.loc = "/home/bartek/programs/spark-1.5.2-bin-hadoop2.6/R/lib/")
sc <- sparkR.init(master = "local", sparkHome = "/home/bartek/programs/spark-1.5.2-bin-hadoop2.6")
sqlContext <- sparkRSQL.init(sc)

library('pipeR')
library('ggplot2')

df <- createDataFrame(sqlContext, mtcars)
model <- glm(mpg ~ wt, data = df, family = "gaussian")
summary(model)
predictions <- predict(model, newData = df)
class(predictions)


predictions %>>%
  select("wt", "mpg", "prediction") %>>%
  collect %>>%
  ggplot() + geom_point(aes(wt, prediction - mpg))  +
  geom_hline(xintercept=0) + theme_bw()
```


```{r}
sparkR.stop()
```

## Contact

bartekskorulski@gmail.com
